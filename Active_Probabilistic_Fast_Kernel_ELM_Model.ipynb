{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "83oC8BngL-1T"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e7qtCBfk1b76"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dat = pd.read_csv(\"Final_Data.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qp6UOCF8SJC6",
    "outputId": "42d05c5d-c228-4f6e-b681-ffe74454d599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "(99975, 55)\n",
      "\n",
      "Testing Data (Non-Identical Ratings):\n",
      "(20941, 55)\n"
     ]
    }
   ],
   "source": [
    "# Define categorical and numerical columns\n",
    "cat_cols = ['STRUCTURE_KIND_043A', 'STRUCTURE_TYPE_043B', 'DECK_STRUCTURE_TYPE_107', 'LOWEST_RATING']\n",
    "num_cols = ['ADT_029', 'MAX_SPAN_LEN_MT_048', 'IMP_LEN_MT_076', 'DECK_AREA', 'TIC', 'Installation_Year']\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "cat_encoded = pd.get_dummies(dat, columns=cat_cols, dtype=float)\n",
    "\n",
    "# Initialize the MinMaxScaler for numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply MinMax scaling to the numerical features\n",
    "cat_encoded[num_cols] = scaler.fit_transform(cat_encoded[num_cols])\n",
    "\n",
    "test_data = cat_encoded[(cat_encoded['CollectionYear'].isin([2021, 2022]))]\n",
    "\n",
    "# Filter the training dataset\n",
    "train_data = cat_encoded[(cat_encoded['CollectionYear'] > 2010) &\n",
    "                        (cat_encoded['CollectionYear'] <= 2020)]\n",
    "\n",
    "# Display summary\n",
    "print(\"Training Data:\")\n",
    "print(train_data.shape)\n",
    "print(\"\\nTesting Data (Non-Identical Ratings):\")\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SAh6-idUBhKz"
   },
   "outputs": [],
   "source": [
    "n_structures_per_query = 10\n",
    "n_queries = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dckTdCtY0qy",
    "outputId": "3a757b1a-636a-4fb7-fc3d-e13980135009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportional weights for each cluster: hierarchical_cluster\n",
      "1    0.004335\n",
      "2    0.163862\n",
      "3    0.109213\n",
      "4    0.684855\n",
      "5    0.002125\n",
      "6    0.000425\n",
      "7    0.035186\n",
      "Name: STRUCTURE_NUMBER_008, dtype: float64\n",
      "Initial bridges per cluster: hierarchical_cluster\n",
      "1    0\n",
      "2    2\n",
      "3    1\n",
      "4    7\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "Name: STRUCTURE_NUMBER_008, dtype: int64\n",
      "Final adjusted bridges per cluster ensuring at least one per cluster: hierarchical_cluster\n",
      "1    1\n",
      "2    2\n",
      "3    1\n",
      "4    7\n",
      "5    1\n",
      "6    1\n",
      "7    1\n",
      "Name: STRUCTURE_NUMBER_008, dtype: int64\n",
      "Selected bridges from cluster 1: ['3351030']\n",
      "Selected bridges from cluster 2: ['3312000', '3346510']\n",
      "Selected bridges from cluster 3: ['4423010']\n",
      "Selected bridges from cluster 4: ['1094190', '3209290', '1050470', '3328150', '1076249', '1052111', '3304970']\n",
      "Selected bridges from cluster 5: ['1077830']\n",
      "Selected bridges from cluster 6: ['5521218']\n",
      "Selected bridges from cluster 7: ['3356470']\n",
      "Initial Set Size: 112\n",
      "Pool Set Size: 99863\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'train_data' is predefined somewhere in your environment\n",
    "total_bridges_in_train_data = train_data['STRUCTURE_NUMBER_008'].nunique()\n",
    "\n",
    "# Calculate the number of unique bridges in each cluster for proportional weights calculation\n",
    "cluster_counts = train_data.groupby('hierarchical_cluster')['STRUCTURE_NUMBER_008'].nunique()\n",
    "proportional_weights = cluster_counts / total_bridges_in_train_data\n",
    "print(\"Proportional weights for each cluster:\", proportional_weights)\n",
    "\n",
    "# Set the total number of samples you want to draw\n",
    "total_samples = n_structures_per_query\n",
    "\n",
    "# Calculate the number of bridges to sample from each cluster based on proportional weights\n",
    "bridges_per_cluster = (proportional_weights * total_samples).round().astype(int)\n",
    "print(\"Initial bridges per cluster:\", bridges_per_cluster)\n",
    "\n",
    "# Ensure the sum of samples equals exactly 20 by adjusting if necessary\n",
    "while bridges_per_cluster.sum() != total_samples:\n",
    "    print(\"Adjusting since total is not 20:\")\n",
    "    if bridges_per_cluster.sum() > total_samples:\n",
    "        max_index = bridges_per_cluster.idxmax()\n",
    "        bridges_per_cluster[max_index] -= 1\n",
    "        print(f\"Reduced by 1 in cluster {max_index}, new distribution:\", bridges_per_cluster)\n",
    "    elif bridges_per_cluster.sum() < total_samples:\n",
    "        min_index = bridges_per_cluster.idxmin()\n",
    "        bridges_per_cluster[min_index] += 1\n",
    "        print(f\"Increased by 1 in cluster {min_index}, new distribution:\", bridges_per_cluster)\n",
    "\n",
    "# Ensure at least one sample from each cluster if possible\n",
    "bridges_per_cluster = bridges_per_cluster.clip(lower=1)\n",
    "print(\"Final adjusted bridges per cluster ensuring at least one per cluster:\", bridges_per_cluster)\n",
    "\n",
    "# Sample randomly from each cluster proportionate to the weights\n",
    "selected_bridges = []\n",
    "for cluster, count in bridges_per_cluster.items():\n",
    "    if count > 0:  # Check if there are bridges to sample\n",
    "        cluster_data = train_data[train_data['hierarchical_cluster'] == cluster]\n",
    "        sample_ids = cluster_data['STRUCTURE_NUMBER_008'].drop_duplicates().sample(n=count, random_state=42)\n",
    "        selected_bridges.extend(sample_ids.tolist())\n",
    "        print(f\"Selected bridges from cluster {cluster}:\", sample_ids.tolist())\n",
    "\n",
    "# Create initial_set by filtering all records of the selected bridge IDs\n",
    "initial_set = train_data[train_data['STRUCTURE_NUMBER_008'].isin(selected_bridges)]\n",
    "print('Initial Set Size:', initial_set.shape[0])\n",
    "\n",
    "# Create pool_set by filtering out the records of the selected bridge IDs\n",
    "pool_set = train_data[~train_data['STRUCTURE_NUMBER_008'].isin(selected_bridges)]\n",
    "print('Pool Set Size:', pool_set.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ey-9u1gfAWx"
   },
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqlNgGaigbI5",
    "outputId": "5bacb6ae-f4ee-4016-a7be-28ffd42035af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(proportional_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUFwRqupOJK7",
    "outputId": "36f24188-fbbb-4bca-fc82-2697e6a0aaea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99863, 55)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pgiiNdeCWYZy"
   },
   "outputs": [],
   "source": [
    "initial_set = initial_set.drop(columns=['CollectionYear'])\n",
    "pool_set = pool_set.drop(columns=['CollectionYear'])\n",
    "test_data = test_data.drop(columns=['CollectionYear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PCCilRvDj6tc"
   },
   "outputs": [],
   "source": [
    "initial_set.loc[:, 'NEXT_LOWEST_RATING'] = initial_set['NEXT_LOWEST_RATING'] - 3\n",
    "pool_set.loc[:, 'NEXT_LOWEST_RATING'] = pool_set['NEXT_LOWEST_RATING'] - 3\n",
    "test_data.loc[:, 'NEXT_LOWEST_RATING'] = test_data['NEXT_LOWEST_RATING'] - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAMjwngpjjH0",
    "outputId": "5301474f-2fd7-4a29-f228-05f455b09db0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 54)\n",
      "(99863, 54)\n",
      "(20941, 54)\n"
     ]
    }
   ],
   "source": [
    "print(initial_set.shape)\n",
    "print(pool_set.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InbR5gHDfLyU"
   },
   "source": [
    "## Fast KELMOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qb08fRuzVzmd"
   },
   "outputs": [],
   "source": [
    "def incomplete_cholesky(g_row,g_diag,K, S, N, epsilon = 1e-5):\n",
    "\n",
    "  pi = list(range(N))\n",
    "  P = np.zeros([S,N])\n",
    "  D = np.copy(g_diag())\n",
    "  err = np.sum(np.abs(D))\n",
    "\n",
    "  s = 0\n",
    "\n",
    "  while(s < S) and (err > epsilon):\n",
    "    i = s + np.argmax([D[pi[j]] for j in range(s,N)])\n",
    "\n",
    "    # line 6 : swap pi[s] and pi[i]\n",
    "\n",
    "    tmp = pi[s]\n",
    "    pi[s] = pi[i]\n",
    "    pi[i] = tmp\n",
    "\n",
    "    # line 7 :\n",
    "    P[s,pi[s]] = np.sqrt(D[pi[s]])\n",
    "    KX = g_row(pi[s])\n",
    "    for i in range(s+1, N):\n",
    "      if s > 0:\n",
    "        inner_p = np.inner(P[:s,pi[s]], P[:s,pi[i]])\n",
    "      else:\n",
    "        inner_p = 0\n",
    "\n",
    "      P[s,pi[i]] = (KX[pi[i]] - inner_p) / P[s,pi[s]]\n",
    "      D[pi[i]] -=  pow(P[s,pi[i]],2)\n",
    "    err = np.sum([D[pi[i]] for i in range(s+1,N)])\n",
    "    s = s + 1\n",
    "\n",
    "  P = P[:s,:]\n",
    "\n",
    "  return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YXZE7KSWfYXq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import kendalltau, somersd\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "\n",
    "\n",
    "class kelmor():\n",
    "    def __init__(self, kernel, C):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        N, F = X.shape\n",
    "        self.t = np.array([[(j-q)**2 for j in range(7)] for q in range(7)])\n",
    "        T = self.t[y, :]\n",
    "        K = pairwise_kernels(X, metric=self.kernel)\n",
    "        g_row = lambda i: K[i, :]\n",
    "        g_diag = lambda: np.diag(K).copy()\n",
    "        N = K.shape[0]\n",
    "        S = 500\n",
    "        P = incomplete_cholesky(g_row, g_diag, K, S, N, epsilon=1e-5)\n",
    "        P = np.transpose(P)\n",
    "        z = self.C * T\n",
    "        u = np.matmul(np.transpose(P), T)\n",
    "        s = np.matmul(np.linalg.inv(np.eye(P.shape[1]) + self.C * np.matmul(np.transpose(P), P)), u)\n",
    "        self.beta = z - self.C**2 * np.matmul(P, s)\n",
    "        return self\n",
    "\n",
    "    def inference(self, X):\n",
    "        K = pairwise_kernels(X, self.X, metric=self.kernel)\n",
    "        fx = np.dot(K, self.beta)\n",
    "        self.y_hat = np.argmin(np.linalg.norm(fx[:, None] - self.t, ord=1, axis=2), axis=1)\n",
    "        NR = -np.linalg.norm(fx[:, None] - self.t, ord=1, axis=2)\n",
    "        self.probs = self.soft_max(NR)\n",
    "        return self.y_hat, self.probs\n",
    "\n",
    "    def soft_max(self, NR):\n",
    "        P = np.exp(NR) / (np.sum(np.exp(NR), axis=1)[:, np.newaxis])\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ubhzwtkogtnz"
   },
   "outputs": [],
   "source": [
    "kel = kelmor(kernel = \"linear\",C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hZGaN8-XddF7"
   },
   "outputs": [],
   "source": [
    "# Define the rps_value function (correct version dont touch)\n",
    "def rps_value(y_pred_prob, y_true):\n",
    "    \"\"\"\n",
    "    Calculate Ranked Probability Score (RPS) for ordinal predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred_prob : numpy array\n",
    "        Array of shape (num_samples, num_categories) containing predicted probabilities for each ordinal category.\n",
    "    y_true : numpy array\n",
    "        Array of shape (num_samples,) containing true ordinal category values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean RPS value across all samples.\n",
    "    \"\"\"\n",
    "    # Convert y_true to numpy array for positional indexing\n",
    "    y_true = np.array(y_true)\n",
    "    num_samples, num_cat = y_pred_prob.shape\n",
    "    # Ensure the true labels match the number of samples\n",
    "    if num_samples != len(y_true):\n",
    "        raise ValueError(f\"Number of samples does not match: {num_samples}, {len(y_true)}\")\n",
    "\n",
    "\n",
    "    # Ensure the true labels match the number of samples\n",
    "    if num_samples != len(y_true):\n",
    "        raise ValueError(f\"Number of samples does not match: {num_samples}, {len(y_true)}\")\n",
    "\n",
    "    # Compute CDFs for predicted probabilities\n",
    "    y_pred_prob_cdf = np.cumsum(y_pred_prob, axis=1)\n",
    "\n",
    "    # Compute CDFs for true labels\n",
    "    y_true_cdf = np.zeros_like(y_pred_prob_cdf)\n",
    "    for k in range(num_samples):\n",
    "        y_true_cdf[k, :] = [(j >= y_true[k]) * 1 for j in range(num_cat)]\n",
    "\n",
    "    # Compute RPS for each sample\n",
    "    rps_per_sample = np.sum((y_pred_prob_cdf - y_true_cdf) ** 2, axis=1) / (num_cat - 1)\n",
    "\n",
    "    # Return the mean RPS value across all samples\n",
    "    return np.round(np.mean(rps_per_sample),4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwYa-V9sfUDs"
   },
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVLKWpkHUHfy",
    "outputId": "e0fa0fc1-62e4-4b3a-8e59-8298ac755426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (112, 51)\n",
      "y_train shape: (112,)\n",
      "X_test shape: (20941, 51)\n",
      "y_test shape: (20941,)\n"
     ]
    }
   ],
   "source": [
    "# Define your feature columns (excluding target, cluster, and structure number)\n",
    "feature_columns = [col for col in initial_set.columns if col not in ['NEXT_LOWEST_RATING', 'hierarchical_cluster', 'STRUCTURE_NUMBER_008']]\n",
    "\n",
    "# Prepare training and testing data\n",
    "X_train = initial_set[feature_columns]\n",
    "y_train = initial_set['NEXT_LOWEST_RATING'].values\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data['NEXT_LOWEST_RATING'].values\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "# Train the initial model\n",
    "kel.fit(X_train.values, y_train)\n",
    "\n",
    "# Generate predicted probabilities for the initial training set\n",
    "_, y_pred_prob_train = kel.inference(X_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFqvnyRyaBTV",
    "outputId": "9c0ce70f-1442-4390-ea1c-a22cda9146ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Ranked Probability Score (RPS): 0.0431\n"
     ]
    }
   ],
   "source": [
    "# Calculate RPS for the initial training set\n",
    "initial_rps = rps_value(y_pred_prob_train, y_train)\n",
    "print(f\"Initial Ranked Probability Score (RPS): {initial_rps:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zPwpouSTbCal"
   },
   "outputs": [],
   "source": [
    "def entropy_sampling_for_structure(kel, pool_set, feature_columns, n_samples, cluster_proportional):\n",
    "    cluster_entropy = {}\n",
    "    for cluster in pool_set['hierarchical_cluster'].unique():\n",
    "        subset = pool_set[pool_set['hierarchical_cluster'] == cluster]\n",
    "        entropies = []\n",
    "        for structure_number in subset['STRUCTURE_NUMBER_008'].unique():\n",
    "            structure_subset = subset[subset['STRUCTURE_NUMBER_008'] == structure_number]\n",
    "            _, probabilities = kel.inference(structure_subset[feature_columns].values)\n",
    "            entropy = -np.sum(probabilities * np.log(probabilities + 1e-5), axis=1).mean()\n",
    "            entropies.append((structure_number, entropy))\n",
    "\n",
    "        # Sort structures in this cluster by descending entropy\n",
    "        entropies.sort(key=lambda x: x[1], reverse=True)\n",
    "        cluster_entropy[cluster] = entropies\n",
    "\n",
    "    # Select structures from each cluster based on their entropy and proportional needs\n",
    "    selected_structures = []\n",
    "    for cluster, entropies in cluster_entropy.items():\n",
    "        n_to_select = max(1, int((n_samples * cluster_proportional.get(cluster, 0))+.5))\n",
    "        selected_structures.extend([structure[0] for structure in entropies[:n_to_select]])\n",
    "\n",
    "    return selected_structures[:n_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Hfei-dbFbCc-"
   },
   "outputs": [],
   "source": [
    "cluster_counts = pool_set['hierarchical_cluster'].value_counts()\n",
    "total_bridges = cluster_counts.sum()\n",
    "cluster_proportional = (cluster_counts / total_bridges).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Metrics - RPS: 0.0340, Precision: 0.7904, Recall: 0.7902, F1 Score: 0.7896\n",
      "Initial Confusion Matrix:\n",
      " [[ 154  108   14    8    1    0    1]\n",
      " [ 117 1320  272   27    6    1    0]\n",
      " [   2  365 4088  944   32    2    2]\n",
      " [   0   17  644 4559  568   15    6]\n",
      " [   0    0    6  143 4179  409   23]\n",
      " [   0    0    1    4  415 1474  119]\n",
      " [   0    0    0    0    0  122  773]]\n",
      "\n",
      "--- Query 1 ---\n",
      "Selected 10 structures with total 68 samples\n",
      "Query 1: RPS = 0.0286, Precision = 0.8199, Recall = 0.8191, F1 Score = 0.8181\n",
      "Confusion Matrix for Query 1:\n",
      " [[ 223   36   22    4    0    0    1]\n",
      " [   0 1571  151   15    6    0    0]\n",
      " [   0  233 4829  322   46    3    2]\n",
      " [   0    0  789 4264  733   19    4]\n",
      " [   0    0    0  573 3822  341   24]\n",
      " [   0    0    0    0  205 1695  113]\n",
      " [   0    0    0    0    0  147  748]]\n",
      "\n",
      "--- Query 2 ---\n",
      "Selected 10 structures with total 62 samples\n",
      "Query 2: RPS = 0.0149, Precision = 0.9142, Recall = 0.9131, F1 Score = 0.9133\n",
      "Confusion Matrix for Query 2:\n",
      " [[ 224   35   19    6    1    0    1]\n",
      " [   1 1576  143   18    5    0    0]\n",
      " [   0   54 5095  256   25    3    2]\n",
      " [   0    0  293 5224  273   13    6]\n",
      " [   0    0    0  134 4393  207   26]\n",
      " [   0    0    0    0   70 1823  120]\n",
      " [   0    0    0    0    0  108  787]]\n",
      "\n",
      "--- Query 3 ---\n",
      "Selected 10 structures with total 60 samples\n",
      "Query 3: RPS = 0.0187, Precision = 0.8910, Recall = 0.8885, F1 Score = 0.8892\n",
      "Confusion Matrix for Query 3:\n",
      " [[ 228   34   17    5    1    0    1]\n",
      " [ 143 1444  133   18    5    0    0]\n",
      " [   0  355 4787  263   26    2    2]\n",
      " [   0    0  288 5229  273   13    6]\n",
      " [   0    0    2  282 4242  206   28]\n",
      " [   0    0    0    0   47 1833  133]\n",
      " [   0    0    0    0    0   51  844]]\n",
      "\n",
      "--- Query 4 ---\n",
      "Selected 10 structures with total 85 samples\n",
      "Query 4: RPS = 0.0163, Precision = 0.9082, Recall = 0.9070, F1 Score = 0.9070\n",
      "Confusion Matrix for Query 4:\n",
      " [[ 225   34   18    7    1    0    1]\n",
      " [  72 1459  183   23    5    1    0]\n",
      " [   0  157 4767  481   24    4    2]\n",
      " [   0    0  108 5379  302   14    6]\n",
      " [   0    0    0   37 4475  220   28]\n",
      " [   0    0    0    0   18 1864  131]\n",
      " [   0    0    0    0    0   71  824]]\n",
      "\n",
      "--- Query 5 ---\n",
      "Selected 10 structures with total 76 samples\n",
      "Query 5: RPS = 0.0111, Precision = 0.9417, Recall = 0.9400, F1 Score = 0.9403\n",
      "Confusion Matrix for Query 5:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1571  150   17    4    1    0]\n",
      " [   0    0 5168  237   24    4    2]\n",
      " [   0    0    6 5501  282   14    6]\n",
      " [   0    0    0    0 4523  210   27]\n",
      " [   0    0    0    0   10 1870  133]\n",
      " [   0    0    0    0    0   70  825]]\n",
      "\n",
      "--- Query 6 ---\n",
      "Selected 10 structures with total 81 samples\n",
      "Query 6: RPS = 0.0108, Precision = 0.9446, Recall = 0.9429, F1 Score = 0.9431\n",
      "Confusion Matrix for Query 6:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5176  230   23    4    2]\n",
      " [   0    0    0 5506  283   14    6]\n",
      " [   0    0    0    4 4519  208   29]\n",
      " [   0    0    0    0    3 1869  141]\n",
      " [   0    0    0    0    0   20  875]]\n",
      "\n",
      "--- Query 7 ---\n",
      "Selected 10 structures with total 70 samples\n",
      "Query 7: RPS = 0.0106, Precision = 0.9458, Recall = 0.9441, F1 Score = 0.9443\n",
      "Confusion Matrix for Query 7:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5178  227   24    4    2]\n",
      " [   0    0    0 5504  285   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1871  142]\n",
      " [   0    0    0    0    0    0  895]]\n",
      "\n",
      "--- Query 8 ---\n",
      "Selected 10 structures with total 80 samples\n",
      "Query 8: RPS = 0.0106, Precision = 0.9455, Recall = 0.9437, F1 Score = 0.9439\n",
      "Confusion Matrix for Query 8:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5178  227   24    4    2]\n",
      " [   0    0    2 5505  282   14    6]\n",
      " [   0    0    0    4 4518  209   29]\n",
      " [   0    0    0    0    0 1871  142]\n",
      " [   0    0    0    0    0    4  891]]\n",
      "\n",
      "--- Query 9 ---\n",
      "Selected 10 structures with total 78 samples\n",
      "Query 9: RPS = 0.0106, Precision = 0.9459, Recall = 0.9441, F1 Score = 0.9443\n",
      "Confusion Matrix for Query 9:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5178  227   24    4    2]\n",
      " [   0    0    0 5507  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1871  142]\n",
      " [   0    0    0    0    0    2  893]]\n",
      "\n",
      "--- Query 10 ---\n",
      "Selected 10 structures with total 71 samples\n",
      "Query 10: RPS = 0.0106, Precision = 0.9459, Recall = 0.9441, F1 Score = 0.9443\n",
      "Confusion Matrix for Query 10:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5178  227   24    4    2]\n",
      " [   0    0    0 5507  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1871  142]\n",
      " [   0    0    0    0    0    2  893]]\n",
      "\n",
      "--- Query 11 ---\n",
      "Selected 10 structures with total 71 samples\n",
      "Query 11: RPS = 0.0106, Precision = 0.9457, Recall = 0.9440, F1 Score = 0.9442\n",
      "Confusion Matrix for Query 11:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    2 5176  227   24    4    2]\n",
      " [   0    0    3 5504  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1871  142]\n",
      " [   0    0    0    0    0    0  895]]\n",
      "\n",
      "--- Query 12 ---\n",
      "Selected 10 structures with total 88 samples\n",
      "Query 12: RPS = 0.0106, Precision = 0.9458, Recall = 0.9440, F1 Score = 0.9442\n",
      "Confusion Matrix for Query 12:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    2 5176  227   24    4    2]\n",
      " [   0    0    2 5505  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1871  142]\n",
      " [   0    0    0    0    0    0  895]]\n",
      "\n",
      "--- Query 13 ---\n",
      "Selected 10 structures with total 86 samples\n",
      "Query 13: RPS = 0.0106, Precision = 0.9458, Recall = 0.9440, F1 Score = 0.9442\n",
      "Confusion Matrix for Query 13:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    2 5176  227   24    4    2]\n",
      " [   0    0    0 5507  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    2 1869  142]\n",
      " [   0    0    0    0    0    0  895]]\n",
      "\n",
      "--- Query 14 ---\n",
      "Selected 10 structures with total 74 samples\n",
      "Query 14: RPS = 0.0106, Precision = 0.9457, Recall = 0.9439, F1 Score = 0.9441\n",
      "Confusion Matrix for Query 14:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1574  147   17    4    1    0]\n",
      " [   0    4 5174  227   24    4    2]\n",
      " [   0    0    2 5505  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1871  142]\n",
      " [   0    0    0    0    0    1  894]]\n",
      "\n",
      "--- Query 15 ---\n",
      "Selected 10 structures with total 81 samples\n",
      "Query 15: RPS = 0.0106, Precision = 0.9457, Recall = 0.9440, F1 Score = 0.9442\n",
      "Confusion Matrix for Query 15:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    2 5176  227   24    4    2]\n",
      " [   0    0    2 5505  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1871  142]\n",
      " [   0    0    0    0    0    1  894]]\n",
      "\n",
      "--- Query 16 ---\n",
      "Selected 10 structures with total 76 samples\n",
      "Query 16: RPS = 0.0106, Precision = 0.9456, Recall = 0.9439, F1 Score = 0.9441\n",
      "Confusion Matrix for Query 16:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    2 5176  227   24    4    2]\n",
      " [   0    0    2 5505  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1873  140]\n",
      " [   0    0    0    0    0    5  890]]\n",
      "\n",
      "--- Query 17 ---\n",
      "Selected 10 structures with total 86 samples\n",
      "Query 17: RPS = 0.0106, Precision = 0.9457, Recall = 0.9440, F1 Score = 0.9442\n",
      "Confusion Matrix for Query 17:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    2 5176  227   24    4    2]\n",
      " [   0    0    2 5505  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1873  140]\n",
      " [   0    0    0    0    0    3  892]]\n",
      "\n",
      "--- Query 18 ---\n",
      "Selected 10 structures with total 73 samples\n",
      "Query 18: RPS = 0.0106, Precision = 0.9458, Recall = 0.9440, F1 Score = 0.9442\n",
      "Confusion Matrix for Query 18:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5178  227   24    4    2]\n",
      " [   0    0    0 5507  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1873  140]\n",
      " [   0    0    0    0    0    6  889]]\n",
      "\n",
      "--- Query 19 ---\n",
      "Selected 10 structures with total 76 samples\n",
      "Query 19: RPS = 0.0106, Precision = 0.9452, Recall = 0.9435, F1 Score = 0.9437\n",
      "Confusion Matrix for Query 19:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5178  227   24    4    2]\n",
      " [   0    0    0 5507  282   14    6]\n",
      " [   0    0    0    0 4522  210   28]\n",
      " [   0    0    0    0    0 1874  139]\n",
      " [   0    0    0    0    0   19  876]]\n",
      "\n",
      "--- Query 20 ---\n",
      "Selected 10 structures with total 81 samples\n",
      "Query 20: RPS = 0.0106, Precision = 0.9455, Recall = 0.9437, F1 Score = 0.9439\n",
      "Confusion Matrix for Query 20:\n",
      " [[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5178  227   24    4    2]\n",
      " [   0    0    2 5505  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1873  140]\n",
      " [   0    0    0    0    0   10  885]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88       286\n",
      "           1       0.98      0.90      0.94      1743\n",
      "           2       0.97      0.95      0.96      5435\n",
      "           3       0.96      0.95      0.95      5809\n",
      "           4       0.94      0.95      0.94      4760\n",
      "           5       0.89      0.93      0.91      2013\n",
      "           6       0.83      0.99      0.90       895\n",
      "\n",
      "    accuracy                           0.94     20941\n",
      "   macro avg       0.94      0.92      0.93     20941\n",
      "weighted avg       0.95      0.94      0.94     20941\n",
      "\n",
      "\n",
      "Classification Report DataFrame:\n",
      "               precision    recall  f1-score       support\n",
      "0              1.000000  0.793706  0.884990    286.000000\n",
      "1              0.980673  0.902467  0.939946   1743.000000\n",
      "2              0.968212  0.952714  0.960401   5435.000000\n",
      "3              0.956560  0.947667  0.952093   5809.000000\n",
      "4              0.935651  0.950000  0.942771   4760.000000\n",
      "5              0.887257  0.930452  0.908341   2013.000000\n",
      "6              0.832549  0.988827  0.903984    895.000000\n",
      "accuracy       0.943747  0.943747  0.943747      0.943747\n",
      "macro avg      0.937272  0.923690  0.927504  20941.000000\n",
      "weighted avg   0.945470  0.943747  0.943941  20941.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "\n",
    "def run_sampling_method(\n",
    "    kel,\n",
    "    X_train, y_train,\n",
    "    pool_set,\n",
    "    X_test, y_test,\n",
    "    n_queries,\n",
    "    feature_columns,\n",
    "    n_structures_per_query\n",
    "):\n",
    "    # Initialize an empty list to store metrics (RPS ,precision, recall, F1 score)\n",
    "    metrics_data = []\n",
    "\n",
    "    # Initial model training and evaluation\n",
    "    kel.fit(X_train.values, y_train)\n",
    "    _, y_pred_probs = kel.inference(X_test.values)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)  # Get the predicted class labels\n",
    "\n",
    "    # Initial metrics calculation\n",
    "    initial_rps = rps_value(y_pred_probs, y_test)\n",
    "    initial_precision, initial_recall, initial_f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='weighted'  \n",
    "    )\n",
    "\n",
    "    # Store initial metrics\n",
    "    metrics_data.append({\n",
    "        'Query': 0,\n",
    "        'RPS': initial_rps,\n",
    "        'Precision': initial_precision,\n",
    "        'Recall': initial_recall,\n",
    "        'F1 Score': initial_f1\n",
    "    })\n",
    "\n",
    "    # Print initial metrics and confusion matrix\n",
    "    print(f\"Initial Metrics - RPS: {initial_rps:.4f}, \"\n",
    "          f\"Precision: {initial_precision:.4f}, Recall: {initial_recall:.4f}, F1 Score: {initial_f1:.4f}\")\n",
    "\n",
    "    # Calculate and print initial confusion matrix\n",
    "    initial_cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Initial Confusion Matrix:\\n\", initial_cm)\n",
    "\n",
    "    for query_index in range(1, n_queries + 1):\n",
    "        print(f\"\\n--- Query {query_index} ---\")\n",
    "\n",
    "        # Calculate cluster proportions for entropy sampling\n",
    "        cluster_counts = pool_set['hierarchical_cluster'].value_counts()\n",
    "        total_bridges = cluster_counts.sum()\n",
    "        cluster_proportional = (cluster_counts / total_bridges).to_dict()\n",
    "\n",
    "        # Get high entropy structure numbers from the entropy sampling method\n",
    "        high_entropy_structure_numbers = entropy_sampling_for_structure(\n",
    "            kel, pool_set, feature_columns, n_structures_per_query, cluster_proportional\n",
    "        )\n",
    "\n",
    "        # Check if new samples are selected based on entropy\n",
    "        if not high_entropy_structure_numbers:\n",
    "            print(\"No new samples selected based on entropy. Stopping active learning.\")\n",
    "            break\n",
    "\n",
    "        # Select new samples based on high entropy structure numbers\n",
    "        new_samples = pool_set[pool_set['STRUCTURE_NUMBER_008'].isin(high_entropy_structure_numbers)]\n",
    "        count = new_samples.shape[0]\n",
    "        print(f\"Selected {len(high_entropy_structure_numbers)} structures with total {count} samples\")\n",
    "\n",
    "        # Update pool set by removing selected samples\n",
    "        pool_set = pool_set[~pool_set['STRUCTURE_NUMBER_008'].isin(high_entropy_structure_numbers)].reset_index(drop=True)\n",
    "\n",
    "        # Extract features and labels from new_samples\n",
    "        X_new = new_samples[feature_columns]\n",
    "        y_new = new_samples['NEXT_LOWEST_RATING'].values\n",
    "\n",
    "        # Append new samples to the training set\n",
    "        X_train = pd.concat([X_train, X_new], ignore_index=True)\n",
    "        y_train = np.concatenate([y_train, y_new])\n",
    "\n",
    "        # Retrain the model with the updated training set\n",
    "        kel.fit(X_train.values, y_train)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        _, y_pred_probs = kel.inference(X_test.values)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)  # Get the predicted class labels\n",
    "\n",
    "        # Calculate metrics for this query\n",
    "        rps = rps_value(y_pred_probs, y_test)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_test, y_pred, average='weighted'  \n",
    "        )\n",
    "\n",
    "        # Store the metrics\n",
    "        metrics_data.append({\n",
    "            'Query': query_index,\n",
    "            'RPS': rps,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1\n",
    "        })\n",
    "\n",
    "        # Print the metrics for this query\n",
    "        print(f\"Query {query_index}: RPS = {rps:.4f}, \"\n",
    "              f\"Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")\n",
    "\n",
    "        # Calculate and print confusion matrix for the current query\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"Confusion Matrix for Query {query_index}:\\n\", cm)\n",
    "\n",
    "    # Convert metrics_data to a DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "    # Save the results to an Excel file\n",
    "    metrics_df.to_excel('active_learning_metrics.xlsx', index=False)\n",
    "\n",
    "    # Return metrics DataFrame, true labels, and predictions\n",
    "    return metrics_df, y_test, y_pred\n",
    "\n",
    "\n",
    "# Running the sampling method\n",
    "metrics_df, y_test, y_pred = run_sampling_method(\n",
    "    kel=kel, \n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    pool_set=pool_set,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    n_queries=n_queries,\n",
    "    feature_columns=feature_columns,\n",
    "    n_structures_per_query=n_structures_per_query\n",
    ")\n",
    "\n",
    "# Generate and print the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "# Save the classification report as a DataFrame\n",
    "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "report_df.to_excel(\"classification_report.xlsx\")\n",
    "\n",
    "# Print the classification report DataFrame\n",
    "print(\"\\nClassification Report DataFrame:\\n\", report_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
