{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "83oC8BngL-1T"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.stats import somersd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e7qtCBfk1b76"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dat = pd.read_csv(\"Final_Data.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qp6UOCF8SJC6",
    "outputId": "42d05c5d-c228-4f6e-b681-ffe74454d599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "(99975, 55)\n",
      "\n",
      "Testing Data (Non-Identical Ratings):\n",
      "(20941, 55)\n"
     ]
    }
   ],
   "source": [
    "# Define categorical and numerical columns\n",
    "cat_cols = ['STRUCTURE_KIND_043A', 'STRUCTURE_TYPE_043B', 'DECK_STRUCTURE_TYPE_107', 'LOWEST_RATING']\n",
    "num_cols = ['ADT_029', 'MAX_SPAN_LEN_MT_048', 'IMP_LEN_MT_076', 'DECK_AREA', 'TIC', 'Installation_Year']\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "cat_encoded = pd.get_dummies(dat, columns=cat_cols, dtype=float)\n",
    "\n",
    "# Initialize the MinMaxScaler for numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply MinMax scaling to the numerical features\n",
    "cat_encoded[num_cols] = scaler.fit_transform(cat_encoded[num_cols])\n",
    "\n",
    "test_data = cat_encoded[(cat_encoded['CollectionYear'].isin([2021, 2022]))]\n",
    "\n",
    "# Filter the training dataset\n",
    "train_data = cat_encoded[(cat_encoded['CollectionYear'] > 2010) &\n",
    "                        (cat_encoded['CollectionYear'] <= 2020)]\n",
    "\n",
    "# Display summary\n",
    "print(\"Training Data:\")\n",
    "print(train_data.shape)\n",
    "print(\"\\nTesting Data (Non-Identical Ratings):\")\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBHaUR95Sb2K",
    "outputId": "dbec8e33-2736-47e7-a01f-2f7729caf344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hierarchical_cluster\n",
      "1      51\n",
      "2    1928\n",
      "3    1285\n",
      "4    8058\n",
      "5      25\n",
      "6       5\n",
      "7     414\n",
      "Name: STRUCTURE_NUMBER_008, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['STRUCTURE_NUMBER_008', 'ADT_029', 'STRUCTURE_KIND_043A',\n",
       "       'STRUCTURE_TYPE_043B', 'MAX_SPAN_LEN_MT_048', 'DECK_STRUCTURE_TYPE_107',\n",
       "       'IMP_LEN_MT_076', 'DECK_AREA', 'LOWEST_RATING', 'NEXT_LOWEST_RATING',\n",
       "       'CollectionYear', 'Installation_Year', 'TIC', 'hierarchical_cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_counts = train_data.groupby('hierarchical_cluster')['STRUCTURE_NUMBER_008'].nunique()\n",
    "\n",
    "print(cluster_counts)\n",
    "dat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the total number of samples you want to draw\n",
    "total_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial bridges per cluster: hierarchical_cluster\n",
      "1    0\n",
      "2    2\n",
      "3    1\n",
      "4    7\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "Name: STRUCTURE_NUMBER_008, dtype: int64\n",
      "Shape of initial_set: (81, 55)\n",
      "Shape of pool_set: (99894, 55)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'train_data' and 'total_samples' are predefined\n",
    "total_bridges_in_train_data = train_data['STRUCTURE_NUMBER_008'].nunique()\n",
    "cluster_counts = train_data.groupby('hierarchical_cluster')['STRUCTURE_NUMBER_008'].nunique()\n",
    "proportional_weights = cluster_counts / total_bridges_in_train_data\n",
    "\n",
    "# Calculate the number of bridges to sample from each cluster based on proportional weights\n",
    "bridges_per_cluster = (proportional_weights * total_samples).round().astype(int)\n",
    "print(\"Initial bridges per cluster:\", bridges_per_cluster)\n",
    "\n",
    "# Sample bridges from each cluster\n",
    "def sample_bridges(group):\n",
    "    n = bridges_per_cluster.get(group.name, 0)\n",
    "    return group.sample(n=n) if n <= len(group) else group.sample(n=len(group))\n",
    "\n",
    "sampled_bridges = train_data.groupby('hierarchical_cluster').apply(sample_bridges).reset_index(drop=True)\n",
    "\n",
    "# Extract Structure Number of sampled bridges\n",
    "sampled_structure_numbers = sampled_bridges['STRUCTURE_NUMBER_008'].unique()\n",
    "\n",
    "# Create initial_set with records of sampled bridges\n",
    "initial_set = train_data[train_data['STRUCTURE_NUMBER_008'].isin(sampled_structure_numbers)]\n",
    "\n",
    "# Create pool_set with the rest of the records\n",
    "pool_set = train_data[~train_data['STRUCTURE_NUMBER_008'].isin(sampled_structure_numbers)]\n",
    "\n",
    "# Print shapes of the datasets\n",
    "print(\"Shape of initial_set:\", initial_set.shape)\n",
    "print(\"Shape of pool_set:\", pool_set.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SAh6-idUBhKz"
   },
   "outputs": [],
   "source": [
    "n_structures_per_query = 10\n",
    "n_queries = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ey-9u1gfAWx"
   },
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqlNgGaigbI5",
    "outputId": "5bacb6ae-f4ee-4016-a7be-28ffd42035af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(proportional_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUFwRqupOJK7",
    "outputId": "36f24188-fbbb-4bca-fc82-2697e6a0aaea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99894, 55)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pgiiNdeCWYZy"
   },
   "outputs": [],
   "source": [
    "initial_set = initial_set.drop(columns=['CollectionYear'])\n",
    "pool_set = pool_set.drop(columns=['CollectionYear'])\n",
    "test_data = test_data.drop(columns=['CollectionYear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PCCilRvDj6tc"
   },
   "outputs": [],
   "source": [
    "initial_set.loc[:, 'NEXT_LOWEST_RATING'] = initial_set['NEXT_LOWEST_RATING'] - 3\n",
    "pool_set.loc[:, 'NEXT_LOWEST_RATING'] = pool_set['NEXT_LOWEST_RATING'] - 3\n",
    "test_data.loc[:, 'NEXT_LOWEST_RATING'] = test_data['NEXT_LOWEST_RATING'] - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAMjwngpjjH0",
    "outputId": "5301474f-2fd7-4a29-f228-05f455b09db0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 54)\n",
      "(99894, 54)\n",
      "(20941, 54)\n"
     ]
    }
   ],
   "source": [
    "print(initial_set.shape)\n",
    "print(pool_set.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InbR5gHDfLyU"
   },
   "source": [
    "## Fast KELMOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qb08fRuzVzmd"
   },
   "outputs": [],
   "source": [
    "def incomplete_cholesky(g_row,g_diag,K, S, N, epsilon = 1e-5):\n",
    "\n",
    "  pi = list(range(N))\n",
    "  P = np.zeros([S,N])\n",
    "  D = np.copy(g_diag())\n",
    "  err = np.sum(np.abs(D))\n",
    "\n",
    "  s = 0\n",
    "\n",
    "  while(s < S) and (err > epsilon):\n",
    "    i = s + np.argmax([D[pi[j]] for j in range(s,N)])\n",
    "\n",
    "    # line 6 : swap pi[s] and pi[i]\n",
    "\n",
    "    tmp = pi[s]\n",
    "    pi[s] = pi[i]\n",
    "    pi[i] = tmp\n",
    "\n",
    "    # line 7 :\n",
    "    P[s,pi[s]] = np.sqrt(D[pi[s]])\n",
    "    KX = g_row(pi[s])\n",
    "    for i in range(s+1, N):\n",
    "      if s > 0:\n",
    "        inner_p = np.inner(P[:s,pi[s]], P[:s,pi[i]])\n",
    "      else:\n",
    "        inner_p = 0\n",
    "\n",
    "      P[s,pi[i]] = (KX[pi[i]] - inner_p) / P[s,pi[s]]\n",
    "      D[pi[i]] -=  pow(P[s,pi[i]],2)\n",
    "    err = np.sum([D[pi[i]] for i in range(s+1,N)])\n",
    "    s = s + 1\n",
    "\n",
    "  P = P[:s,:]\n",
    "\n",
    "  return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YXZE7KSWfYXq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import kendalltau, somersd\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "\n",
    "\n",
    "class kelmor():\n",
    "    def __init__(self, kernel, C):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        N, F = X.shape\n",
    "        self.t = np.array([[(j-q)**2 for j in range(7)] for q in range(7)])\n",
    "        T = self.t[y, :]\n",
    "        K = pairwise_kernels(X, metric=self.kernel)\n",
    "        g_row = lambda i: K[i, :]\n",
    "        g_diag = lambda: np.diag(K).copy()\n",
    "        N = K.shape[0]\n",
    "        S = 500\n",
    "        P = incomplete_cholesky(g_row, g_diag, K, S, N, epsilon=1e-5)\n",
    "        P = np.transpose(P)\n",
    "        z = self.C * T\n",
    "        u = np.matmul(np.transpose(P), T)\n",
    "        s = np.matmul(np.linalg.inv(np.eye(P.shape[1]) + self.C * np.matmul(np.transpose(P), P)), u)\n",
    "        self.beta = z - self.C**2 * np.matmul(P, s)\n",
    "        return self\n",
    "\n",
    "    def inference(self, X):\n",
    "        K = pairwise_kernels(X, self.X, metric=self.kernel)\n",
    "        fx = np.dot(K, self.beta)\n",
    "        self.y_hat = np.argmin(np.linalg.norm(fx[:, None] - self.t, ord=1, axis=2), axis=1)\n",
    "        NR = -np.linalg.norm(fx[:, None] - self.t, ord=1, axis=2)\n",
    "        self.probs = self.soft_max(NR)\n",
    "        return self.y_hat, self.probs\n",
    "\n",
    "    def soft_max(self, NR):\n",
    "        P = np.exp(NR) / (np.sum(np.exp(NR), axis=1)[:, np.newaxis])\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Ubhzwtkogtnz"
   },
   "outputs": [],
   "source": [
    "#kel = kelmor(kernel = \"rbf\",C = 0.5)\n",
    "kel = kelmor(kernel = \"linear\",C = 5) # best by defined grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hZGaN8-XddF7"
   },
   "outputs": [],
   "source": [
    "# Define the rps_value function (correct version dont touch)\n",
    "def rps_value(y_pred_prob, y_true):\n",
    "    \"\"\"\n",
    "    Calculate Ranked Probability Score (RPS) for ordinal predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred_prob : numpy array\n",
    "        Array of shape (num_samples, num_categories) containing predicted probabilities for each ordinal category.\n",
    "    y_true : numpy array\n",
    "        Array of shape (num_samples,) containing true ordinal category values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean RPS value across all samples.\n",
    "    \"\"\"\n",
    "    # Convert y_true to numpy array for positional indexing\n",
    "    y_true = np.array(y_true)\n",
    "    num_samples, num_cat = y_pred_prob.shape\n",
    "    # Ensure the true labels match the number of samples\n",
    "    if num_samples != len(y_true):\n",
    "        raise ValueError(f\"Number of samples does not match: {num_samples}, {len(y_true)}\")\n",
    "\n",
    "\n",
    "    # Ensure the true labels match the number of samples\n",
    "    if num_samples != len(y_true):\n",
    "        raise ValueError(f\"Number of samples does not match: {num_samples}, {len(y_true)}\")\n",
    "\n",
    "    # Compute CDFs for predicted probabilities\n",
    "    y_pred_prob_cdf = np.cumsum(y_pred_prob, axis=1)\n",
    "\n",
    "    # Compute CDFs for true labels\n",
    "    y_true_cdf = np.zeros_like(y_pred_prob_cdf)\n",
    "    for k in range(num_samples):\n",
    "        y_true_cdf[k, :] = [(j >= y_true[k]) * 1 for j in range(num_cat)]\n",
    "\n",
    "    # Compute RPS for each sample\n",
    "    rps_per_sample = np.sum((y_pred_prob_cdf - y_true_cdf) ** 2, axis=1) / (num_cat - 1)\n",
    "\n",
    "    # Return the mean RPS value across all samples\n",
    "    return np.round(np.mean(rps_per_sample),4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwYa-V9sfUDs"
   },
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVLKWpkHUHfy",
    "outputId": "e0fa0fc1-62e4-4b3a-8e59-8298ac755426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (81, 51)\n",
      "y_train shape: (81,)\n",
      "X_test shape: (20941, 51)\n",
      "y_test shape: (20941,)\n"
     ]
    }
   ],
   "source": [
    "# Define your feature columns (excluding target, cluster, and structure number)\n",
    "feature_columns = [col for col in initial_set.columns if col not in ['NEXT_LOWEST_RATING', 'hierarchical_cluster', 'STRUCTURE_NUMBER_008']]\n",
    "\n",
    "# Prepare training and testing data\n",
    "X_train = initial_set[feature_columns]\n",
    "y_train = initial_set['NEXT_LOWEST_RATING'].values\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data['NEXT_LOWEST_RATING'].values\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "# Train the initial model\n",
    "kel.fit(X_train.values, y_train)\n",
    "\n",
    "# Generate predicted probabilities for the initial training set\n",
    "_, y_pred_prob_train = kel.inference(X_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zPwpouSTbCal"
   },
   "outputs": [],
   "source": [
    "def entropy_sampling_for_structure(kel, pool_set, feature_columns, n_samples, cluster_proportional):\n",
    "    cluster_entropy = {}\n",
    "    for cluster in pool_set['hierarchical_cluster'].unique():\n",
    "        subset = pool_set[pool_set['hierarchical_cluster'] == cluster]\n",
    "        entropies = []\n",
    "        for structure_number in subset['STRUCTURE_NUMBER_008'].unique():\n",
    "            structure_subset = subset[subset['STRUCTURE_NUMBER_008'] == structure_number]\n",
    "            _, probabilities = kel.inference(structure_subset[feature_columns].values)\n",
    "            entropy = -np.sum(probabilities * np.log(probabilities + 1e-5), axis=1).mean()\n",
    "            entropies.append((structure_number, entropy))\n",
    "\n",
    "        # Sort structures in this cluster by descending entropy\n",
    "        entropies.sort(key=lambda x: x[1], reverse=True)\n",
    "        cluster_entropy[cluster] = entropies\n",
    "\n",
    "    # Select structures from each cluster based on their entropy and proportional needs\n",
    "    selected_structures = []\n",
    "    for cluster, entropies in cluster_entropy.items():\n",
    "        n_to_select = max(1, int((n_samples * cluster_proportional.get(cluster, 0))+.5))\n",
    "        selected_structures.extend([structure[0] for structure in entropies[:n_to_select]])\n",
    "\n",
    "    return selected_structures[:n_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Hfei-dbFbCc-"
   },
   "outputs": [],
   "source": [
    "cluster_counts = pool_set['hierarchical_cluster'].value_counts()\n",
    "total_bridges = cluster_counts.sum()\n",
    "cluster_proportional = (cluster_counts / total_bridges).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DezWQxY7bCfN",
    "outputId": "304897b7-38e4-4974-c61f-8278886cc500"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Metrics - RPS: 0.1095, Accuracy: 0.5485, Precision: 0.5231, Recall: 0.5485, F1-score: 0.5223, Somers' D: 0.4480\n",
      "Initial Confusion Matrix:\n",
      "[[   0    0   14  229   34    8    1]\n",
      " [   0    0  111 1306  250   76    0]\n",
      " [   0    0 3660  981  749   43    2]\n",
      " [   0    0   60 4342 1069  332    6]\n",
      " [   0    0  512 2428 1646  145   29]\n",
      " [   0    0    0  249  678  944  142]\n",
      " [   0    0    0    0    0    0  895]]\n",
      "\n",
      "--- Query 1 ---\n",
      "Selected 10 structures with total 59 samples\n",
      "Query 1: RPS = 0.0421, Accuracy = 0.7704, Precision = 0.7392, Recall = 0.7704, F1-score = 0.7447, Somers' D: 0.8233\n",
      "Confusion Matrix for Query 1:\n",
      "[[ 121  109   31   23    1    0    1]\n",
      " [   0   52 1187  467   37    0    0]\n",
      " [   0  156 4341  915   19    2    2]\n",
      " [   0    0  279 5256  256   12    6]\n",
      " [   0    0   38  743 3753  197   29]\n",
      " [   0    0    0    1  154 1716  142]\n",
      " [   0    0    0    0    0    2  893]]\n",
      "\n",
      "--- Query 2 ---\n",
      "Selected 10 structures with total 80 samples\n",
      "Query 2: RPS = 0.0156, Accuracy = 0.9090, Precision = 0.9104, Recall = 0.9090, F1-score = 0.9089, Somers' D: 0.9448\n",
      "Confusion Matrix for Query 2:\n",
      "[[ 220   37   22    5    1    0    1]\n",
      " [   0 1528  193   18    4    0    0]\n",
      " [   0   44 5049  314   24    2    2]\n",
      " [   0    0  165 5363  262   13    6]\n",
      " [   0    0    2  327 4206  196   29]\n",
      " [   0    0    0    0   97 1774  142]\n",
      " [   0    0    0    0    0    0  895]]\n",
      "\n",
      "--- Query 3 ---\n",
      "Selected 10 structures with total 79 samples\n",
      "Query 3: RPS = 0.0123, Accuracy = 0.9314, Precision = 0.9327, Recall = 0.9314, F1-score = 0.9315, Somers' D: 0.9611\n",
      "Confusion Matrix for Query 3:\n",
      "[[ 226   33   19    6    1    0    1]\n",
      " [   0 1557  164   17    4    1    0]\n",
      " [   0   90 5080  235   24    4    2]\n",
      " [   0    0   43 5468  278   14    6]\n",
      " [   0    0    0   74 4452  205   29]\n",
      " [   0    0    0    0   45 1826  142]\n",
      " [   0    0    0    0    0    0  895]]\n",
      "\n",
      "--- Query 4 ---\n",
      "Selected 10 structures with total 80 samples\n",
      "Query 4: RPS = 0.0106, Accuracy = 0.9440, Precision = 0.9457, Recall = 0.9440, F1-score = 0.9442, Somers' D: 0.9678\n",
      "Confusion Matrix for Query 4:\n",
      "[[ 226   32   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5176  229   24    4    2]\n",
      " [   0    0    2 5505  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1871  142]\n",
      " [   0    0    0    0    0    0  895]]\n",
      "\n",
      "--- Query 5 ---\n",
      "Selected 10 structures with total 66 samples\n",
      "Query 5: RPS = 0.0149, Accuracy = 0.9135, Precision = 0.9266, Recall = 0.9135, F1-score = 0.9042, Somers' D: 0.9587\n",
      "Confusion Matrix for Query 5:\n",
      "[[ 227   31   20    6    1    1    0]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5178  227   24    5    1]\n",
      " [   0    0    0 5507  282   19    1]\n",
      " [   0    0    0    0 4522  236    2]\n",
      " [   0    0    0    0    0 2004    9]\n",
      " [   0    0    0    0    0  776  119]]\n",
      "\n",
      "--- Query 6 ---\n",
      "Selected 10 structures with total 79 samples\n",
      "Query 6: RPS = 0.0106, Accuracy = 0.9439, Precision = 0.9457, Recall = 0.9439, F1-score = 0.9441, Somers' D: 0.9677\n",
      "Confusion Matrix for Query 6:\n",
      "[[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5176  229   24    4    2]\n",
      " [   0    0    0 5507  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    2 1869  142]\n",
      " [   0    0    0    0    0    2  893]]\n",
      "\n",
      "--- Query 7 ---\n",
      "Selected 10 structures with total 74 samples\n",
      "Query 7: RPS = 0.0108, Accuracy = 0.9425, Precision = 0.9442, Recall = 0.9425, F1-score = 0.9426, Somers' D: 0.9670\n",
      "Confusion Matrix for Query 7:\n",
      "[[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5168  237   25    3    2]\n",
      " [   0    0    0 5494  296   14    5]\n",
      " [   0    0    0    0 4523  208   29]\n",
      " [   0    0    0    0    9 1865  139]\n",
      " [   0    0    0    0    0    9  886]]\n",
      "\n",
      "--- Query 8 ---\n",
      "Selected 10 structures with total 79 samples\n",
      "Query 8: RPS = 0.0106, Accuracy = 0.9437, Precision = 0.9455, Recall = 0.9437, F1-score = 0.9439, Somers' D: 0.9676\n",
      "Confusion Matrix for Query 8:\n",
      "[[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5174  231   24    4    2]\n",
      " [   0    0    3 5504  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1873  140]\n",
      " [   0    0    0    0    0    5  890]]\n",
      "\n",
      "--- Query 9 ---\n",
      "Selected 10 structures with total 80 samples\n",
      "Query 9: RPS = 0.0107, Accuracy = 0.9435, Precision = 0.9453, Recall = 0.9435, F1-score = 0.9437, Somers' D: 0.9675\n",
      "Confusion Matrix for Query 9:\n",
      "[[ 227   31   20    6    1    0    1]\n",
      " [   0 1571  150   17    4    1    0]\n",
      " [   0    0 5172  233   24    4    2]\n",
      " [   0    0    0 5504  285   14    6]\n",
      " [   0    0    0    2 4518  211   29]\n",
      " [   0    0    0    0    0 1871  142]\n",
      " [   0    0    0    0    0    0  895]]\n",
      "\n",
      "--- Query 10 ---\n",
      "Selected 10 structures with total 83 samples\n",
      "Query 10: RPS = 0.0106, Accuracy = 0.9442, Precision = 0.9460, Recall = 0.9442, F1-score = 0.9444, Somers' D: 0.9679\n",
      "Confusion Matrix for Query 10:\n",
      "[[ 227   31   20    6    1    0    1]\n",
      " [   0 1573  148   17    4    1    0]\n",
      " [   0    0 5178  227   24    4    2]\n",
      " [   0    0    0 5507  282   14    6]\n",
      " [   0    0    0    0 4522  209   29]\n",
      " [   0    0    0    0    0 1871  142]\n",
      " [   0    0    0    0    0    0  895]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def run_sampling_method(\n",
    "    kel,\n",
    "    X_train, y_train,\n",
    "    pool_set,\n",
    "    X_test, y_test,\n",
    "    n_queries,\n",
    "    feature_columns,\n",
    "    n_structures_per_query\n",
    "):\n",
    "    # Initialize lists to store metrics at each step\n",
    "    RPS_entropy_sampling = []\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    somers_d_list = []\n",
    "    confusion_matrices = []  # List to store confusion matrices\n",
    "\n",
    "    # Initial model training and evaluation\n",
    "    kel.fit(X_train.values, y_train)\n",
    "    _, y_pred_probs = kel.inference(X_test.values)\n",
    "    \n",
    "    # Convert probabilities to class labels\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    # Compute metrics\n",
    "    initial_rps = rps_value(y_pred_probs, y_test)\n",
    "    initial_accuracy = accuracy_score(y_test, y_pred)\n",
    "    initial_precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    initial_recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    initial_f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    initial_somers_d = somersd(y_test, y_pred).correlation\n",
    "    initial_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Store initial metrics\n",
    "    RPS_entropy_sampling.append(initial_rps)\n",
    "    accuracy_list.append(initial_accuracy)\n",
    "    precision_list.append(initial_precision)\n",
    "    recall_list.append(initial_recall)\n",
    "    f1_list.append(initial_f1)\n",
    "    somers_d_list.append(initial_somers_d)\n",
    "    confusion_matrices.append(initial_confusion_matrix)\n",
    "\n",
    "    print(f\"Initial Metrics - RPS: {initial_rps:.4f}, Accuracy: {initial_accuracy:.4f}, \"\n",
    "          f\"Precision: {initial_precision:.4f}, Recall: {initial_recall:.4f}, \"\n",
    "          f\"F1-score: {initial_f1:.4f}, Somers' D: {initial_somers_d:.4f}\")\n",
    "    print(f\"Initial Confusion Matrix:\\n{initial_confusion_matrix}\")\n",
    "\n",
    "    for query_index in range(1, n_queries + 1):\n",
    "        print(f\"\\n--- Query {query_index} ---\")\n",
    "\n",
    "        # Calculate cluster proportions for entropy sampling\n",
    "        cluster_counts = pool_set['hierarchical_cluster'].value_counts()\n",
    "        total_bridges = cluster_counts.sum()\n",
    "        cluster_proportional = (cluster_counts / total_bridges).to_dict()\n",
    "\n",
    "        high_entropy_structure_numbers = entropy_sampling_for_structure(kel, pool_set, feature_columns, n_structures_per_query, cluster_proportional)\n",
    "\n",
    "        if not high_entropy_structure_numbers:\n",
    "            print(\"No new samples selected based on entropy. Stopping active learning.\")\n",
    "            break\n",
    "\n",
    "        # Select new samples based on high entropy structure numbers\n",
    "        new_samples = pool_set[pool_set['STRUCTURE_NUMBER_008'].isin(high_entropy_structure_numbers)]\n",
    "        count = new_samples.shape[0]\n",
    "        print(f\"Selected {len(high_entropy_structure_numbers)} structures with total {count} samples\")\n",
    "\n",
    "        # Update pool set by removing selected samples\n",
    "        pool_set = pool_set[~pool_set['STRUCTURE_NUMBER_008'].isin(high_entropy_structure_numbers)].reset_index(drop=True)\n",
    "\n",
    "        # Extract features and labels from new_samples\n",
    "        X_new = new_samples[feature_columns]\n",
    "        y_new = new_samples['NEXT_LOWEST_RATING'].values\n",
    "\n",
    "        # Append new samples to the training set\n",
    "        X_train = pd.concat([X_train, X_new], ignore_index=True)\n",
    "        y_train = np.concatenate([y_train, y_new])\n",
    "\n",
    "        # Retrain the model with the updated training set\n",
    "        kel.fit(X_train.values, y_train)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        _, y_pred_probs = kel.inference(X_test.values)\n",
    "\n",
    "        # Convert probabilities to class labels\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "        # Compute metrics\n",
    "        rps = rps_value(y_pred_probs, y_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "        recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        somers_d_value = somersd(y_test, y_pred).correlation\n",
    "        confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Store results\n",
    "        RPS_entropy_sampling.append(rps)\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        somers_d_list.append(somers_d_value)\n",
    "        confusion_matrices.append(confusion_mat)\n",
    "\n",
    "        # Print results for the query\n",
    "        print(f\"Query {query_index}: RPS = {rps:.4f}, Accuracy = {accuracy:.4f}, \"\n",
    "              f\"Precision = {precision:.4f}, Recall = {recall:.4f}, \"\n",
    "              f\"F1-score = {f1:.4f}, Somers' D: {somers_d_value:.4f}\")\n",
    "        print(f\"Confusion Matrix for Query {query_index}:\\n{confusion_mat}\")\n",
    "\n",
    "    return {\n",
    "        \"RPS\": RPS_entropy_sampling,\n",
    "        \"Accuracy\": accuracy_list,\n",
    "        \"Precision\": precision_list,\n",
    "        \"Recall\": recall_list,\n",
    "        \"F1-score\": f1_list,\n",
    "        \"Somers' D\": somers_d_list,\n",
    "        \"Confusion Matrices\": confusion_matrices  # Add confusion matrices to the return\n",
    "    }\n",
    "\n",
    "# Run the function and store results in a dictionary\n",
    "metrics_entropy_sampling = run_sampling_method(\n",
    "    kel=kel,  # Model instance\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    pool_set=pool_set,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    n_queries=n_queries,\n",
    "    feature_columns=feature_columns,  # List of columns to be used for feature extraction\n",
    "    n_structures_per_query=n_structures_per_query  # Number of unique structures to select per query\n",
    ")\n",
    "\n",
    "# Extract individual lists\n",
    "rps_list = metrics_entropy_sampling[\"RPS\"]\n",
    "accuracy_list = metrics_entropy_sampling[\"Accuracy\"]\n",
    "precision_list = metrics_entropy_sampling[\"Precision\"]\n",
    "recall_list = metrics_entropy_sampling[\"Recall\"]\n",
    "f1_list = metrics_entropy_sampling[\"F1-score\"]\n",
    "somers_d_list = metrics_entropy_sampling[\"Somers' D\"]\n",
    "confusion_matrices = metrics_entropy_sampling[\"Confusion Matrices\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>RPS</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Somers' D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.548541</td>\n",
       "      <td>0.523076</td>\n",
       "      <td>0.548541</td>\n",
       "      <td>0.522272</td>\n",
       "      <td>0.447975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.770355</td>\n",
       "      <td>0.739178</td>\n",
       "      <td>0.770355</td>\n",
       "      <td>0.744685</td>\n",
       "      <td>0.823265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.908982</td>\n",
       "      <td>0.910421</td>\n",
       "      <td>0.908982</td>\n",
       "      <td>0.908894</td>\n",
       "      <td>0.944848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.931379</td>\n",
       "      <td>0.932658</td>\n",
       "      <td>0.931379</td>\n",
       "      <td>0.931450</td>\n",
       "      <td>0.961144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.943985</td>\n",
       "      <td>0.945720</td>\n",
       "      <td>0.943985</td>\n",
       "      <td>0.944152</td>\n",
       "      <td>0.967761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Query     RPS  Accuracy  Precision    Recall  F1-score  Somers' D\n",
       "0      0  0.1095  0.548541   0.523076  0.548541  0.522272   0.447975\n",
       "1      1  0.0421  0.770355   0.739178  0.770355  0.744685   0.823265\n",
       "2      2  0.0156  0.908982   0.910421  0.908982  0.908894   0.944848\n",
       "3      3  0.0123  0.931379   0.932658  0.931379  0.931450   0.961144\n",
       "4      4  0.0106  0.943985   0.945720  0.943985  0.944152   0.967761"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excel = pd.read_excel('entropy_sampling_metrics.xlsx')\n",
    "excel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuP1TLaKfSZL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPLgguFAfSbV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
